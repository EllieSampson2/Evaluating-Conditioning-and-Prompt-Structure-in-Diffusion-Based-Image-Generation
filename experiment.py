# -*- coding: utf-8 -*-
"""Experiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M_2NEAOSYe-FICO_kc6XkmSAkzY3COJg

Setup & Imports
"""

!pip install diffusers transformers accelerate torch torchvision ftfy scipy pillow open_clip_torch
import torch
from diffusers import StableDiffusionPipeline
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import open_clip

"""Model Loading"""

device = "cuda" if torch.cuda.is_available() else "cpu"

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
).to(device)

pipe.enable_attention_slicing()

"""Prompt Sets (Controlled Variations)"""

base_prompt = "A modern poster design for a coffee shop"

prompt_variants = {
    "base": base_prompt,
    "explicit_layout": base_prompt + ", centered layout, clean typography",
    "style_tokens": base_prompt + ", minimalist, flat design, vector art",
    "structured": "Poster design. Title text at top. Image in center. Minimalist style.",
    "overloaded": base_prompt + ", ultra-detailed, cinematic lighting, 8k, photorealistic"
}

"""Image Generation Loop"""

def generate(prompt, seed):
    generator = torch.Generator(device=device).manual_seed(seed)
    return pipe(prompt, generator=generator, num_inference_steps=30).images[0]

results = {}

for name, prompt in prompt_variants.items():
    images = [generate(prompt, seed=i) for i in range(3)]
    results[name] = images

"""Quantitative Evaluation"""

def show_grid(images, title):
    fig, axes = plt.subplots(1, len(images), figsize=(12,4))
    for ax, img in zip(axes, images):
        ax.imshow(img)
        ax.axis("off")
    fig.suptitle(title)
    plt.show()

for k, imgs in results.items():
    show_grid(imgs, k)

"""Quantitative Analysis"""

model, _, preprocess = open_clip.create_model_and_transforms(
    'ViT-B-32', pretrained='openai'
)
tokenizer = open_clip.get_tokenizer('ViT-B-32')

model = model.to(device)

def clip_score(image, text):
    image = preprocess(image).unsqueeze(0).to(device)
    text = tokenizer([text]).to(device)
    with torch.no_grad():
        image_feat = model.encode_image(image)
        text_feat = model.encode_text(text)
    return torch.cosine_similarity(image_feat, text_feat).item()

scores = {}

for k, imgs in results.items():
    scores[k] = np.mean([clip_score(img, base_prompt) for img in imgs])

scores

"""Findings & Next Steps

### Findings

- Explicit layout prompts achieved the highest CLIP similarity, indicating improved semantic alignment when spatial and typographic cues are included.

- Base and overloaded prompts showed comparable CLIP scores, suggesting that generic quality tokens increase visual complexity without meaningfully improving alignment.

- Structured prompts produced the lowest CLIP similarity despite improved layout consistency, highlighting a tradeoff between rigid structural guidance and semantic fidelity.

- Style-focused prompts preserved semantic alignment while increasing aesthetic consistency, but did not outperform explicit layout conditioning.

- Overall, prompt structure meaningfully affects both alignment and composition, acting as a soft prior that trades off flexibility for control.

### Limitations
- Experiments were limited to a single pretrained model (Stable Diffusion v1.5).
- CLIP similarity does not directly capture layout or typography quality.
- No fine-tuning, ControlNet, or multimodal conditioning was explored.

### Next Steps
- Incorporate ControlNet or other spatial conditioning methods to decouple layout control from semantic alignment.

- Compare Stable Diffusion v1.5 with SDXL to assess improvements in layout and typography fidelity.

- Evaluate typography and layout quality using task-specific or human-in-the-loop metrics beyond CLIP.

- Analyze variance across seeds to quantify diversityâ€“consistency tradeoffs more rigorously.

- Extend experiments to multimodal conditioning (e.g., layout sketches or bounding boxes) to better support design workflows.
"""